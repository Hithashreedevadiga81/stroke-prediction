# -*- coding: utf-8 -*-
"""Copy of  FINALpredictionandanalysisofstroke.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G2jZqAOo1odlhQBbqJQFFNLGEDCud6eX

about the dataset:
according to the World Health Organisation stroke is the 2nd leading cause of death globally, responsible for approx 11% of total deaths. this dataset is used to predict the likelihood of getting a stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relevant information about the patient.

**DATA PROCESSING AND ANALYSIS**
"""

import numpy as np #array processing
import pandas as pd #data processing
import os #Data Importing
import matplotlib.pyplot as plt #plots
import seaborn as sns #graphs

"""**PRE PROCESSING**"""

from sklearn.preprocessing import FunctionTransformer #transforming of data
from sklearn.preprocessing  import OneHotEncoder  #Data encoding
from sklearn.preprocessing  import StandardScaler # data scaling
from imblearn.over_sampling import RandomOverSampler #data oversampling
from sklearn.decomposition import PCA #principal component analysis

"""**models**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

"""**neural networks**"""

import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import layers, models

"""**matrics**"""

from sklearn.metrics import accuracy_score , classification_report #model classification report
from sklearn.metrics import roc_curve, auc

"""**READING A DATA SET**"""

df=pd.read_csv('/content/healthcare-dataset-stroke-data.csv')

df

df.head()

df.tail()

type(df)

"""**EXPLORING THE DATASET**"""

df.shape

df.dtypes

df.info()

df.describe()

df.shape

df.ndim

df.stroke.unique

df.stroke.value_counts().rename('count'),
df.stroke.value_counts(True).rename('%').mul(100)

df["stroke"].value_counts()

# show number of patients by a stroke
sns.countplot(data=df , x= 'stroke')
plt.title('number of Patients')

sns.countplot(data=df , x= 'gender')
plt.title('number of Patients')

sns.countplot(data=df , x= 'age')
plt.title('number of Patients')

"""**FEATURE NAMES**"""

df.columns

df["stroke"].value_counts()

"""**MISSING VALUES**"""

df.isnull()

df.isnull().any()

df.isnull().sum()

print('missing data sum:')
print(df.isnull().sum())

print('missing data percentage (%):')
print(df.isnull().sum()/df.count()*100)

"""**SEPERATE THE CATEGORICAL AND NUMERICAL FEATURES**


"""

cat_features = [feature for feature in df.columns if df[feature].dtypes =='O']
print('number of categorical variables:',len(cat_features))
print('categorical variable colums name:' , cat_features)

cd = pd.DataFrame(cat_features)

cd.head()

numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O' ]
print('number of numerical variables:',len(numerical_features))
print('numerical variable colums :' ,numerical_features)

numerical_features

cat_features

"""**CHECKING FOR UNIQUE VALUES**"""

df.gender.duplicated()

df.duplicated().sum() #overall data check

# data exploring
df['gender'].unique()

df['age'].nunique()

df['hypertension'].unique()

df['heart_disease'].unique()

df['ever_married'].unique()

df['work_type'].unique()

df['Residence_type'].unique()

df['avg_glucose_level'].unique()

df['bmi'].nunique()

df['smoking_status'].unique()

df['stroke'].unique()

#correlation matrix
corr = df.corr()
plt.figure(figsize=(8,8))
sns.heatmap(data=corr,annot=True , cmap='Spectral').set(title='correlation matrix')

corr = df.corr()
fig = plt.figure(figsize=(12,8))
sns.heatmap(corr,linewidths=.5 , cmap="RdBu",annot=True ,fmt="g" )

corr_matrix = df.corr().round(2)
corr_matrix

mask = np.triu(np.ones_like(corr_matrix,dtype=bool))
plt.figure(figsize=(10,10))
sns.heatmap(corr_matrix,center=0,vmin=-1,vmax=1,mask=mask,annot=True,cmap='BrBG')

"""**VISUALISING THE CATEGORICAL FEATURES**"""

for col in cat_features[:]:
  plt.figure(figsize=(6,3),dpi=100)
  sns.countplot(data=df,x=col,hue='stroke',palette='gist_rainbow_r')
  plt.legend(loc=(1.05,0.5))

"""**BARPLOT OF NUMERICAL FEATURE**"""

for col in numerical_features:
  plt.figure(figsize=(6,3),dpi=100)
  sns.barplot(data=df,x='stroke', y=col ,palette='gist_rainbow_r')

"""**HANDLING THE MISSING VALUES**"""

df.isnull().sum()

df["bmi"]=df["bmi"].fillna(df["bmi"].mean())

df.isnull().sum()

"""**DROPPING THE IRRELEVANT FEATURE "ID"**"""

train = df.drop(['id'], axis=1)
train

train.columns

train.shape

train.info()

train_data_cat=train.select_dtypes("object")
train_data_num=train.select_dtypes("number")

train_data_cat.head()

train_data_num.head()

"""**CONVERTING A CATEGORICAL FEATURE INTO A NUMERICAL FEATURES**"""

train_data_cata_encoded=pd.get_dummies(train_data_cat , columns=train_data_cat.columns.to_list())
train_data_cata_encoded.head()

data=pd.concat([train_data_cata_encoded,train_data_num],axis=1,join="outer")
data.head()

"""**SEPERATE DEPENDENT AND INDEPENDENT FEATURE**"""

y=data['stroke']
x=data.drop('stroke',axis=1)

print(x.shape)
print(y.shape)

"""**SCALING A DATA**"""

sc = StandardScaler()
x = sc.fit_transform(x)

x

# Standardize the data (important for PCA)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Apply PCA to reduce dimensionality
pca = PCA(n_components=10)  # Adjust the number of components based on your requirements
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)
# Create a RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the PCA-transformed data
model.fit(X_train_pca, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_pca)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
matrix = confusion_matrix(y_test, y_pred)

# Print or visualize the results as needed
print("Accuracy:", accuracy)
print("Classification Report:\n", report)
print("Confusion Matrix:\n", matrix)

"""Accuracy: 0.76

The overall accuracy of your model on the test set is 76%. Accuracy is the ratio of correctly predicted instances to the total instances.
Classification Report:

Precision: The precision is the ratio of correctly predicted positive observations to the total predicted positives. For class 0 (no stroke), it's 0.69, and for class 1 (stroke), it's 0.86. This means that when the model predicts a positive class, it's correct 86% of the time for class 1 and 69% for class 0.
Recall (Sensitivity): Recall is the ratio of correctly predicted positive observations to the all observations in the actual class. For class 0, it's 0.88, and for class 1, it's 0.65. This means that the model correctly identifies 88% of the actual instances of class 0 and 65% of the actual instances of class 1.
F1-Score: The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. For class 0, it's 0.77, and for class 1, it's 0.74.
Support: The number of actual occurrences of the class in the specified dataset. For class 0, there are 93 instances, and for class 1, there are 107 instances.
Confusion Matrix:

The confusion matrix provides a more detailed breakdown of correct and incorrect predictions:
###True Positive (TP): 70 instances of class 1 correctly predicted as class 1.
###True Negative (TN): 82 instances of class 0 correctly predicted as class 0.
###False Positive (FP): 11 instances of class 0 incorrectly predicted as class 1.
###False Negative (FN): 37 instances of class 1 incorrectly predicted as class 0.

# **MODEL SELECTION**

**SPLTTING A DATA INTO TRAINING SET AND TESTING SET**
"""

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=7)
x_train.shape,x_test.shape

y_train = y_train.ravel()

"""# **BULDING CLASSIFIER**"""

accuracy = {}

"""**LOGISTIC REGRESSION**"""

lr = LogisticRegression(max_iter=200)
lr.fit(x_train, y_train)
y_predl = lr.predict(x_test)
print(accuracy_score(y_test,y_predl))
accuracy[str(lr)] = accuracy_score(y_test,y_predl)*100

"""**GRADIENT BOOTING CLASIIFIER**"""

gbc = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1)
gbc.fit(x_train,y_train)
y_pred4 = gbc.predict(x_test)
print(accuracy_score(y_test,y_pred4))
accuracy[str(gbc)] = accuracy_score(y_test,y_pred4)*100

"""**RANDOM FOREST CLASSIFIER**"""

# Create the classifier object
rf_clf = RandomForestClassifier(n_estimators = 100)

# Train the model using the training sets
rf_clf.fit(x_train, y_train)

# performing predictions on the test dataset
y_pred_rf = rf_clf.predict(x_test)

# Printing accuracy of the model
print('Accuracy:', accuracy_score(y_test,y_pred_rf))

"""**CONFUSION MATRIX**"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np

# Assuming you have already trained your random forest classifier (rf_clf) and have the test data (x_test, y_test)
# Make predictions using the trained model
y_pred = rf_clf.predict(x_test)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_clf.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
classifier = LogisticRegression()
classifier.fit(X_train, y_train)
y_scores = classifier.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, color='darkred', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.text(0.6, 0.2, 'AUC = {:.2f}'.format(roc_auc), bbox=dict(facecolor='white', alpha=0.5))

plt.show()

"""**DECISION TREE CLASSIFIER**"""

# Create a decision tree classifier
clf = DecisionTreeClassifier()
# Train the classifier on the training set
clf.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = clf.predict(X_test)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Generate a classification report
report = classification_report(y_test, y_pred ,output_dict=True)

# Print the classification report
print("Classification Report:\n", report)

# Convert the classification report to a DataFrame for easier plotting
report_df = pd.DataFrame(report).transpose()

# Plot the classification report
plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:-1, :].T, annot=True, cmap='Blues', fmt=".2f", linewidths=.5)
plt.title('Classification Report Heatmap')
plt.show()

# Define the hyperparameter grid
param_dist = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a RandomForestClassifier
classifier = RandomForestClassifier()

# Create the RandomizedSearchCV object
random_search = RandomizedSearchCV(classifier, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)


# Fit the randomized search to the data
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = random_search.best_estimator_

# Make predictions on the test set
y_pred = best_model.predict(X_test)

"""If your search space is small, grid search might be appropriate. If the space is large, or you have limited computational resources, randomized search is a more efficient choice. Adjust the parameters and search spaces based on your specific requirements."""

# Build a simple feedforward neural network using Keras
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Display the model summary
model.summary()

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

# Make predictions on the test set
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Print or visualize the results as needed

"""TensorFlow's Keras provides a high-level API that simplifies the process of building and training neural networks, making it more accessible and user-friendly."""

# Build a simple feedforward neural network using Sequential
model = Sequential()

# Add an input layer with the same number of neurons as features
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))

# Add a hidden layer with 32 neurons
model.add(Dense(32, activation='relu'))

# Add an output layer with one neuron and a sigmoid activation function for binary classification
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Display the model summary
model.summary()

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

# Make predictions on the test set
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

"""# **OUTPUT**"""

input_data = (0,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,67.0,0,1,228.69,36.600000)
# change the input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the numpy array as we are predicting for one datapoint
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = lr.predict(input_data_reshaped)
print(prediction)

if (prediction[0] == 0):
  print('there is no stroke')

else:
  print('There is stroke')